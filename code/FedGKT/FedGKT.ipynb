{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmUJVmbleOkA"
      },
      "source": [
        "# Implementation of FedGKT framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwdYMUM7eO1o"
      },
      "outputs": [],
      "source": [
        "# Import all the necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torchvision import transforms as tt\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import time\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "sns.set()\n",
        "\n",
        "# set manual seed for reproducibility\n",
        "# [100, 0, 42] => list of seeds\n",
        "seed = 0\n",
        "\n",
        "# general reproducibility\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# gpu training specific\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkAZXf-OLyfi"
      },
      "source": [
        "## Argument Parser initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxn-jX4jTQRP"
      },
      "outputs": [],
      "source": [
        "def add_args(parser):\n",
        "  # Training settings\n",
        "  parser.add_argument('--model_client', type=str, default='resnet5', metavar='N', \\\n",
        "                      help='neural network used in training')\n",
        "\n",
        "  parser.add_argument('--model_server', type=str, default='resnet32', metavar='N', \\\n",
        "                      help='neural network used in training')\n",
        "  # Dataset used\n",
        "  parser.add_argument('--dataset', type=str, default='cifar10', metavar='N', \\\n",
        "                      help='dataset used for training')\n",
        "  # Path for dataset storage\n",
        "  parser.add_argument('--data_dir', type=str, default='./content', help='data directory') \n",
        "  # how to partition the dataset on local workers\n",
        "  parser.add_argument('--partition_method', type=str, default='hetero', metavar='N', \\\n",
        "                      help='how to partition the dataset on local workers')\n",
        "  \n",
        "  parser.add_argument('--partition_alpha', type=float, default=0.5, metavar='PA', \\\n",
        "                      help='partition alpha (default: 0.5)')\n",
        "\n",
        "  parser.add_argument('--batch_size', type=int, default=128, metavar='N', \\\n",
        "                      help='input batch size for training (default: 64)')\n",
        "\n",
        "  parser.add_argument('--lr', type=float, default=0.001, metavar='LR', \\\n",
        "                      help='learning rate (default: 0.001)')\n",
        "\n",
        "  parser.add_argument('--wd', help='weight decay parameter;', type=float, default=1e-4)\n",
        "\n",
        "  parser.add_argument('--epochs_client', type=int, default=1, metavar='EP', \\\n",
        "                      help='how many epochs will be trained locally')\n",
        "\n",
        "  parser.add_argument('--local_points', type=int, default=5000, metavar='LP', \\\n",
        "                      help='the approximate fixed number of data points we will have on each local worker')\n",
        "  # Client's number\n",
        "  parser.add_argument('--client_number', type=int, default=100, metavar='NN', \\\n",
        "                      help='number of workers in a distributed cluster')\n",
        "\n",
        "  parser.add_argument('--comm_round', type=int, default=20, \\\n",
        "                      help='how many round of communications we shoud use')\n",
        "\n",
        "  parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
        "\n",
        "  parser.add_argument('--loss_scale', type=float, default=1024, help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
        "\n",
        "  parser.add_argument('--no_bn_wd', action='store_true', help='Remove batch norm from weight decay')\n",
        "\n",
        "  # knowledge distillation\n",
        "  parser.add_argument('--temperature', default=3.0, type=float, help='Input the temperature: default(3.0)')\n",
        "  parser.add_argument('--epochs_server', type=int, default=5, metavar='EP', help='how many epochs will be trained on the server side')\n",
        "  # [0, 0.1, 0.01]\n",
        "  parser.add_argument('--alpha', default=1, type=float, help='Input the relative weight: default(1.0)')\n",
        "  parser.add_argument('--optimizer', default=\"SGD\", type=str, help='optimizer: SGD, Adam, etc.')\n",
        "  parser.add_argument('--whether_training_on_client', default=1, type=int)\n",
        "  parser.add_argument('--whether_distill_on_the_server', default=0, type=int)\n",
        "  parser.add_argument('--client_model', default=\"resnet4\", type=str)\n",
        "  parser.add_argument('--weight_init_model', default=\"resnet32\", type=str)\n",
        "  parser.add_argument('--running_name', default=\"default\", type=str)\n",
        "  parser.add_argument('--sweep', default=0, type=int)\n",
        "  parser.add_argument('--multi_gpu_server', action='store_true')\n",
        "  parser.add_argument('--test', action='store_true', help='test mode, only run 1-2 epochs to test the bug of the program')\n",
        "  parser.add_argument('--gpu_num_per_server', type=int, default=1, help='gpu_num_per_server')\n",
        "  # quick solution to solve the parsing problem\n",
        "  parser.add_argument('-f')\n",
        "  args = parser.parse_args()\n",
        "  return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjLRyUiWLt9z"
      },
      "outputs": [],
      "source": [
        "# parse python script input parameters\n",
        "parser = argparse.ArgumentParser()\n",
        "args = add_args(parser)\n",
        "logging.info(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WePKs5Hdi9SG"
      },
      "source": [
        "## Class CIFAR-10 Truncated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0zP8BIfi9ZQ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    return pil_loader(path)\n",
        "\n",
        "\n",
        "class CIFAR10_truncated(data.Dataset):\n",
        "\n",
        "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
        "\n",
        "        self.root = root\n",
        "        self.dataidxs = dataidxs\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.download = download\n",
        "\n",
        "        self.data, self.target = self.__build_truncated_dataset__()\n",
        "\n",
        "    # Splitting between data and target\n",
        "    def __build_truncated_dataset__(self):\n",
        "        print(\"download = \" + str(self.download))\n",
        "        cifar_dataobj = CIFAR10(self.root, self.train, self.transform, self.target_transform, self.download)\n",
        "\n",
        "        if self.train:\n",
        "            # print(\"train member of the class: {}\".format(self.train))\n",
        "            # data = cifar_dataobj.train_data\n",
        "            data = cifar_dataobj.data\n",
        "            target = np.array(cifar_dataobj.targets)\n",
        "        else:\n",
        "            data = cifar_dataobj.data\n",
        "            target = np.array(cifar_dataobj.targets)\n",
        "\n",
        "        if self.dataidxs is not None:\n",
        "            data = data[self.dataidxs]\n",
        "            target = target[self.dataidxs]\n",
        "\n",
        "        return data, target\n",
        "\n",
        "    def truncate_channel(self, index):\n",
        "        for i in range(index.shape[0]):\n",
        "            gs_index = index[i]\n",
        "            self.data[gs_index, :, :, 1] = 0.0\n",
        "            self.data[gs_index, :, :, 2] = 0.0\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.target[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdpeKkcHXVoV"
      },
      "source": [
        "## Dataset initialization + Dataloaders definition + Dataset Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwoyNZVutD_7"
      },
      "source": [
        "### DataLoaders definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6I05yxBtDGn"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def record_net_data_stats(y_train, net_dataidx_map):\n",
        "    net_cls_counts = {}\n",
        "\n",
        "    for net_i, dataidx in net_dataidx_map.items():\n",
        "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n",
        "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
        "        net_cls_counts[net_i] = tmp\n",
        "    logging.debug('Data statistics: %s' % str(net_cls_counts))\n",
        "    return net_cls_counts\n",
        "    \n",
        "def get_dataloader(dataset, datadir, train_bs, test_bs, dataidxs=None):\n",
        "    return get_dataloader_CIFAR10(datadir, train_bs, test_bs, dataidxs)\n",
        "\n",
        "def get_dataloader_CIFAR10(datadir, train_bs, test_bs, dataidxs=None):\n",
        "    \"\"\"\n",
        "      :param datadir: data directory\n",
        "      :param train_bs: batch size for training\n",
        "      :param test_bs:  batch size for test\n",
        "      :param dataidxs: indexes of the splitting\n",
        "      :return: train DataLoader, test DataLoader\n",
        "    \"\"\"\n",
        "    dl_obj = CIFAR10_truncated\n",
        "\n",
        "    transform_train, transform_test = _data_transforms_cifar10()\n",
        "\n",
        "    train_ds = dl_obj(datadir, dataidxs=dataidxs, train=True, transform=transform_train, download=True)\n",
        "    test_ds = dl_obj(datadir, train=False, transform=transform_test, download=True)\n",
        "\n",
        "    train_dl = data.DataLoader(dataset=train_ds, batch_size=train_bs, shuffle=True, drop_last=True)\n",
        "    test_dl = data.DataLoader(dataset=test_ds, batch_size=test_bs, shuffle=False, drop_last=True)\n",
        "\n",
        "    return train_dl, test_dl\n",
        "\n",
        "class Cutout(object):\n",
        "    def __init__(self, length):\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "\n",
        "        y1 = np.clip(y - self.length // 2, 0, h)\n",
        "        y2 = np.clip(y + self.length // 2, 0, h)\n",
        "        x1 = np.clip(x - self.length // 2, 0, w)\n",
        "        x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "        mask[y1: y2, x1: x2] = 0.\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img *= mask\n",
        "        return img\n",
        "\n",
        "def _data_transforms_cifar10():\n",
        "    \"\"\"\n",
        "        Defines the transformations to apply to the data.\n",
        "        return: transformations for train and validation dataaset\n",
        "    \"\"\"\n",
        "    CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
        "    CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
        "    ])\n",
        "\n",
        "    train_transform.transforms.append(Cutout(16))\n",
        "\n",
        "    valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
        "    ])\n",
        "\n",
        "    return train_transform, valid_transform\n",
        "\n",
        "def load_cifar10_data(datadir):\n",
        "    \"\"\"\n",
        "      :param data directory\n",
        "      :return: splits (x_train, y_train, x_test, y_test)\n",
        "    \"\"\"\n",
        "    train_transform, test_transform = _data_transforms_cifar10()\n",
        "    # Build distributed datasets\n",
        "    cifar10_train_ds = CIFAR10_truncated(datadir, train=True, download=True, transform=train_transform)\n",
        "    cifar10_test_ds = CIFAR10_truncated(datadir, train=False, download=True, transform=test_transform)\n",
        "\n",
        "    X_train, y_train = cifar10_train_ds.data, cifar10_train_ds.target\n",
        "    X_test, y_test = cifar10_test_ds.data, cifar10_test_ds.target\n",
        "\n",
        "    return (X_train, y_train, X_test, y_test)\n",
        "\n",
        "def partition_data(dataset, datadir, partition, n_nets, alpha):\n",
        "    \"\"\"\n",
        "      :param dataset: dataset name\n",
        "      :param datadir: storage folder\n",
        "      :param partition: partition method (homogeneous as default)\n",
        "      :param n_nets: number of clients\n",
        "      :param alpha: proptionality in different partitions\n",
        "      :return: splits: X_train, y_train, X_test, y_test\n",
        "      \n",
        "    \"\"\"\n",
        "    logging.info(\"*********partition data***************\")\n",
        "    X_train, y_train, X_test, y_test = load_cifar10_data(datadir)\n",
        "    n_train = X_train.shape[0]\n",
        "    # n_test = X_test.shape[0]\n",
        "\n",
        "    if partition == \"homo\":\n",
        "        total_num = n_train\n",
        "        idxs = np.random.permutation(total_num)\n",
        "        batch_idxs = np.array_split(idxs, n_nets)\n",
        "        net_dataidx_map = {i: batch_idxs[i] for i in range(n_nets)}\n",
        "\n",
        "    elif partition == \"hetero\":\n",
        "        min_size = 0\n",
        "        K = 10\n",
        "        N = y_train.shape[0]\n",
        "        logging.info(\"N = \" + str(N))\n",
        "        net_dataidx_map = {}\n",
        "\n",
        "        while min_size < 10:\n",
        "            idx_batch = [[] for _ in range(n_nets)]\n",
        "            # for each class in the dataset\n",
        "            for k in range(K):\n",
        "                idx_k = np.where(y_train == k)[0]\n",
        "                np.random.shuffle(idx_k)\n",
        "                # Here we use alpha for partion the dataset\n",
        "                proportions = np.random.dirichlet(np.repeat(alpha, n_nets))\n",
        "                ## Balance\n",
        "                proportions = np.array([p * (len(idx_j) < N / n_nets) for p, idx_j in zip(proportions, idx_batch)])\n",
        "                proportions = proportions / proportions.sum()\n",
        "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
        "                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]\n",
        "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
        "\n",
        "        for j in range(n_nets):\n",
        "            np.random.shuffle(idx_batch[j])\n",
        "            net_dataidx_map[j] = idx_batch[j]\n",
        "            \n",
        "    # Probably we will never use those features, but we can hold for future experiments\n",
        "    elif partition == \"hetero-fix\":\n",
        "        dataidx_map_file_path = './data_preprocessing/non-iid-distribution/CIFAR10/net_dataidx_map.txt'\n",
        "        net_dataidx_map = read_net_dataidx_map(dataidx_map_file_path)\n",
        "\n",
        "    if partition == \"hetero-fix\":\n",
        "        distribution_file_path = './data_preprocessing/non-iid-distribution/CIFAR10/distribution.txt'\n",
        "        traindata_cls_counts = read_data_distribution(distribution_file_path)\n",
        "    else:\n",
        "        traindata_cls_counts = record_net_data_stats(y_train, net_dataidx_map)\n",
        "\n",
        "    # net_dataidx_map is a dictionary of length 4: {key: int, value: [list of indexes mapping the data among the workers}\n",
        "    # traindata_cls_counts is a dictionary of length 4, basically assesses how the different labels are distributed among\n",
        "    # the client, counting the total number of examples per class in each client.\n",
        "    return X_train, y_train, X_test, y_test, net_dataidx_map, traindata_cls_counts\n",
        "\n",
        "def load_partition_data_cifar10(dataset, data_dir, partition_method, partition_alpha, client_number, batch_size):\n",
        "    \"\"\"\n",
        "      :param dataset: dataset name\n",
        "      :param data_dir: storage folder\n",
        "      :param partition_method: partition method used (homogeneous as default)\n",
        "      :param partition_alpha: constant regulation the proportions in the partitions (default 0.5)\n",
        "      :param client_number: number of clients involved \n",
        "      :param batch_size: number of images in a batch\n",
        "      :return: partitions\n",
        "    \"\"\"\n",
        "    X_train, y_train, X_test, y_test, net_dataidx_map, traindata_cls_counts = partition_data(dataset, data_dir, partition_method, client_number, partition_alpha)\n",
        "\n",
        "    class_num = len(np.unique(y_train))\n",
        "    logging.info(\"traindata_cls_counts = \" + str(traindata_cls_counts))\n",
        "    train_data_num = sum([len(net_dataidx_map[r]) for r in range(client_number)])\n",
        "\n",
        "    train_data_global, test_data_global = get_dataloader(dataset, data_dir, batch_size, batch_size)\n",
        "    logging.info(\"train_dl_global number = \" + str(len(train_data_global)))\n",
        "    logging.info(\"test_dl_global number = \" + str(len(test_data_global)))\n",
        "    test_data_num = len(test_data_global)\n",
        "\n",
        "    # get local dataset\n",
        "    data_local_num_dict = dict()\n",
        "    train_data_local_dict = dict()\n",
        "    test_data_local_dict = dict()\n",
        "\n",
        "    for client_idx in range(client_number):\n",
        "        dataidxs = net_dataidx_map[client_idx]\n",
        "        local_data_num = len(dataidxs)\n",
        "        data_local_num_dict[client_idx] = local_data_num\n",
        "        logging.info(\"client_idx = %d, local_sample_number = %d\" % (client_idx, local_data_num))\n",
        "\n",
        "        # training batch size = 64; algorithms batch size = 32\n",
        "        train_data_local, test_data_local = get_dataloader(dataset, data_dir, batch_size, batch_size,\n",
        "                                                           dataidxs)\n",
        "        logging.info(\"client_idx = %d, batch_num_train_local = %d, batch_num_test_local = %d\" % (\n",
        "            client_idx, len(train_data_local), len(test_data_local)))\n",
        "        train_data_local_dict[client_idx] = train_data_local\n",
        "        test_data_local_dict[client_idx] = test_data_local\n",
        "    return train_data_num, test_data_num, train_data_global, test_data_global, data_local_num_dict, \\\n",
        "    train_data_local_dict, test_data_local_dict, class_num #, traindata_cls_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsjBz-BWdxlR"
      },
      "source": [
        "### Dataset partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "68f14a0e3ef34785ad7c45e4c9e611a2",
            "3711899d965c480188b327e0911b1934",
            "4806bb2f379e45e2ae9dbb933815d19a",
            "2f26ca9dbbe04d79b1140df43ddb90c8",
            "97d6a7c41535463a8f5a886b846c276b",
            "ce994710836f4daba76e8f39b184b8d3",
            "bac1b63264f54e67b76efd714af51156",
            "85d98484143843d09799844b81061597",
            "d106d88d32f54eefbf4e3737752ab6bf",
            "32aaa31f04d5450dba4a0925521a4dee",
            "fbc4685cb7c149d4973aa962e84136ca"
          ]
        },
        "id": "kVZJBFnCWTUd",
        "outputId": "a1a970d2-7336-4694-997f-89e81705f07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:*********partition data***************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./content/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68f14a0e3ef34785ad7c45e4c9e611a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./content/cifar-10-python.tar.gz to ./content\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:N = 50000\n",
            "INFO:root:traindata_cls_counts = {0: {0: 86, 1: 69, 2: 2, 3: 34, 4: 83, 5: 17, 6: 21, 7: 209}, 1: {0: 2, 1: 28, 2: 37, 3: 135, 4: 11, 5: 62, 7: 132, 8: 47, 9: 318}, 2: {0: 1, 1: 3, 2: 6, 3: 3, 5: 7, 6: 2, 7: 114, 8: 5, 9: 35}, 3: {0: 36, 1: 1, 2: 21, 3: 70, 5: 47, 6: 7, 7: 5, 8: 226, 9: 5}, 4: {0: 46, 1: 54, 2: 46, 3: 37, 4: 8, 5: 296, 6: 82}, 5: {0: 114, 1: 123, 2: 68, 3: 49, 4: 8, 5: 132, 6: 89}, 6: {0: 1, 2: 32, 3: 24, 4: 180, 5: 1, 7: 16, 8: 166, 9: 54}, 7: {0: 66, 2: 9, 3: 38, 4: 32, 5: 8, 6: 24, 8: 1, 9: 30}, 8: {0: 103, 1: 34, 2: 1, 3: 86, 4: 55, 5: 5, 6: 105, 7: 45}, 9: {1: 10, 2: 250, 3: 1, 4: 21, 5: 24, 6: 186, 7: 12}, 10: {0: 106, 1: 19, 3: 182, 4: 15, 5: 114, 6: 82}, 11: {0: 47, 2: 103, 3: 1, 4: 19, 5: 36, 6: 1, 7: 1, 8: 314}, 12: {0: 42, 1: 45, 2: 56, 3: 12, 5: 60, 6: 86, 7: 95, 8: 6}, 13: {0: 45, 1: 80, 2: 87, 3: 59, 4: 33, 5: 2, 6: 27, 7: 296}, 14: {0: 82, 1: 26, 2: 22, 3: 4, 4: 18, 5: 17, 6: 231, 7: 19, 8: 9, 9: 197}, 15: {0: 24, 1: 120, 2: 23, 3: 42, 4: 82, 5: 123, 6: 162}, 16: {0: 5, 2: 44, 3: 28, 4: 77, 5: 19, 6: 1, 7: 42, 8: 18, 9: 115}, 17: {0: 1, 1: 2, 2: 35, 3: 49, 4: 27, 5: 96, 6: 10, 7: 11, 8: 25, 9: 15}, 18: {0: 5, 2: 59, 3: 31, 4: 30, 5: 23, 6: 15, 7: 109, 8: 13, 9: 24}, 19: {0: 6, 1: 18, 2: 53, 3: 62, 4: 15, 5: 47, 6: 40, 7: 22, 8: 9, 9: 353}, 20: {0: 2, 1: 25, 5: 117, 6: 23, 7: 1}, 21: {0: 13, 1: 1, 2: 121, 3: 4, 4: 15, 5: 37, 7: 10, 8: 102, 9: 268}, 22: {0: 40, 1: 221, 2: 60, 3: 143, 4: 4, 5: 119}, 23: {1: 48, 2: 2, 3: 24, 4: 18, 5: 19, 6: 461}, 24: {0: 20, 1: 271, 2: 105, 3: 61, 4: 551}, 25: {0: 97, 1: 28, 2: 36, 3: 43, 4: 64, 5: 1, 7: 20, 8: 4, 9: 416}, 26: {0: 111, 1: 15, 2: 17, 3: 27, 4: 110, 5: 1, 6: 5, 7: 149, 8: 56, 9: 46}, 27: {0: 56, 1: 234, 2: 59, 3: 4, 4: 79, 5: 52, 7: 4, 8: 87}, 28: {0: 2, 1: 5, 2: 148, 3: 1, 4: 17, 5: 13, 6: 191, 7: 61, 8: 63}, 29: {0: 40, 1: 2, 2: 64, 4: 299, 5: 148}, 30: {0: 11, 1: 95, 2: 12, 3: 4, 4: 52, 5: 38, 6: 16, 8: 183, 9: 171}, 31: {0: 273, 1: 23, 2: 19, 3: 107, 4: 5, 5: 18, 6: 17, 7: 50}, 32: {0: 193, 1: 135, 2: 58, 3: 120}, 33: {0: 12, 1: 2, 2: 31, 3: 124, 4: 123, 5: 5, 7: 310}, 34: {0: 124, 1: 23, 2: 5, 3: 107, 4: 4, 5: 454}, 35: {0: 481, 2: 58}, 36: {0: 195, 1: 15, 3: 3, 4: 16, 5: 39, 6: 1, 7: 303}, 37: {0: 9, 1: 128, 2: 5, 3: 77, 4: 123, 5: 1, 6: 54, 7: 39, 8: 100}, 38: {0: 31, 1: 229, 2: 215, 3: 2, 4: 88}, 39: {0: 76, 1: 1, 2: 5, 3: 16, 4: 2, 5: 90, 7: 13, 8: 25, 9: 100}, 40: {0: 32, 1: 19, 3: 214, 4: 140, 5: 3, 6: 8, 7: 10, 8: 344}, 41: {0: 5, 1: 47, 2: 10, 3: 34, 4: 62, 5: 173, 6: 317}, 42: {0: 8, 1: 22, 2: 55, 3: 138, 4: 29, 5: 32, 6: 113, 7: 20, 8: 32}, 43: {0: 4, 1: 2, 2: 1, 3: 1, 4: 65, 5: 19, 6: 2, 7: 13, 8: 22, 9: 154}, 44: {0: 13, 1: 26, 2: 9, 3: 120, 4: 2, 5: 3, 6: 2, 7: 141, 9: 9}, 45: {0: 2, 1: 83, 2: 3, 3: 20, 4: 26, 6: 13, 7: 7, 8: 32, 9: 149}, 46: {0: 8, 1: 22, 2: 7, 3: 4, 4: 98, 5: 4, 6: 106, 7: 136, 8: 196}, 47: {0: 17, 1: 1, 2: 76, 3: 21, 4: 70, 5: 4, 6: 83, 7: 29, 8: 5, 9: 61}, 48: {0: 5, 1: 159, 2: 128, 3: 2, 4: 13, 5: 55, 6: 74, 7: 72}, 49: {0: 103, 1: 29, 2: 2, 3: 1, 4: 45, 5: 8, 6: 116, 7: 19, 8: 217}, 50: {0: 80, 1: 1, 2: 40, 3: 4, 4: 201, 5: 201}, 51: {0: 123, 1: 21, 2: 59, 3: 27, 4: 29, 5: 194, 6: 138}, 52: {1: 3, 2: 491, 3: 137}, 53: {0: 3, 1: 71, 2: 7, 3: 47, 4: 36, 5: 2, 6: 45, 7: 11, 8: 58, 9: 19}, 54: {1: 146, 2: 69, 3: 12, 4: 99, 5: 15, 6: 15, 7: 1, 8: 7, 9: 78}, 55: {0: 30, 1: 57, 2: 15, 4: 25, 5: 8, 6: 96, 7: 105, 8: 52, 9: 17}, 56: {0: 54, 1: 87, 2: 13, 3: 116, 4: 32, 5: 113, 6: 250}, 57: {0: 5, 2: 34, 3: 93, 4: 8, 5: 1, 6: 173, 7: 83, 8: 31, 9: 85}, 58: {0: 12, 1: 24, 2: 4, 3: 192, 4: 82, 5: 9, 6: 2, 7: 100, 8: 24, 9: 15}, 59: {0: 65, 2: 154, 3: 19, 4: 91, 5: 34, 6: 2, 7: 134, 8: 136}, 60: {0: 56, 1: 3, 2: 1, 3: 202, 4: 74, 5: 43, 6: 3, 7: 3, 8: 146}, 61: {0: 18, 1: 89, 2: 14, 3: 277, 4: 3, 5: 4, 6: 19, 7: 71, 8: 22}, 62: {0: 20, 1: 2, 2: 39, 3: 34, 4: 29, 5: 217, 6: 35, 7: 4, 8: 113, 9: 77}, 63: {0: 282, 1: 47, 2: 1, 3: 28, 5: 43, 6: 4, 7: 206}, 64: {0: 30, 1: 41, 2: 3, 3: 1, 4: 5, 5: 18, 6: 1, 7: 23, 8: 17, 9: 2}, 65: {0: 2, 1: 2, 3: 27, 4: 1, 5: 140, 6: 20, 7: 8, 8: 15, 9: 1}, 66: {0: 2, 1: 60, 2: 29, 3: 2, 4: 2, 5: 42, 6: 11, 7: 35, 9: 81}, 67: {0: 72, 1: 3, 2: 15, 3: 17, 4: 19, 6: 150, 7: 9, 8: 1, 9: 7}, 68: {0: 10, 1: 20, 2: 4, 3: 22, 4: 164, 5: 5, 6: 1, 7: 101, 8: 205}, 69: {0: 4, 1: 63, 3: 16, 5: 17, 6: 24, 7: 135, 8: 198}, 70: {0: 115, 2: 11, 4: 30, 5: 33, 6: 11, 7: 239, 8: 5, 9: 2}, 71: {1: 26, 2: 102, 3: 58, 4: 2, 5: 1, 6: 6, 7: 3, 8: 7, 9: 21}, 72: {0: 6, 1: 114, 2: 130, 3: 4, 4: 18, 6: 91, 7: 3, 8: 216}, 73: {0: 92, 2: 3, 3: 13, 5: 50, 6: 136, 7: 6, 8: 7, 9: 61}, 74: {0: 28, 2: 2, 3: 3, 4: 138, 5: 56, 6: 78, 7: 184, 8: 331}, 75: {0: 5, 1: 1, 2: 124, 3: 36, 4: 25, 5: 1, 6: 3, 7: 294, 8: 4, 9: 166}, 76: {0: 162, 1: 95, 2: 12, 3: 54, 4: 44, 5: 6, 6: 78, 7: 39, 8: 102}, 77: {0: 199, 1: 30, 2: 157, 3: 14, 4: 22, 5: 20, 6: 16, 7: 3, 8: 6, 9: 24}, 78: {0: 20, 1: 185, 2: 10, 3: 193, 4: 4, 5: 3, 6: 8, 7: 7, 8: 206}, 79: {0: 1, 1: 87, 2: 168, 3: 31, 4: 24, 5: 114, 6: 50, 7: 57}, 80: {0: 43, 1: 4, 2: 1, 3: 177, 4: 13, 5: 8, 6: 2, 7: 28, 8: 22, 9: 24}, 81: {0: 32, 1: 51, 2: 6, 3: 3, 4: 36, 5: 75, 7: 9, 8: 1, 9: 340}, 82: {0: 3, 1: 9, 2: 34, 3: 28, 4: 129, 5: 8, 6: 1, 7: 46, 8: 75, 9: 1}, 83: {0: 165, 1: 24, 2: 3, 3: 129, 4: 70, 5: 29, 7: 14, 8: 1, 9: 227}, 84: {0: 6, 1: 25, 2: 44, 3: 73, 4: 1, 5: 9, 6: 4, 7: 124, 9: 34}, 85: {0: 108, 1: 323, 2: 5, 3: 1, 4: 29, 5: 10, 6: 16, 8: 65}, 86: {0: 12, 2: 10, 3: 3, 4: 42, 5: 77, 6: 21, 7: 9, 8: 45, 9: 215}, 87: {0: 17, 1: 12, 2: 21, 3: 26, 4: 21, 5: 28, 6: 4, 7: 164, 8: 95, 9: 67}, 88: {0: 27, 1: 19, 2: 25, 3: 19, 4: 3, 5: 9, 6: 76, 8: 15, 9: 96}, 89: {0: 131, 1: 8, 2: 147, 3: 50, 5: 122, 6: 1, 8: 14, 9: 14}, 90: {0: 43, 1: 298, 2: 5, 3: 80, 4: 8, 5: 4, 6: 50, 7: 18}, 91: {0: 21, 1: 27, 2: 18, 4: 118, 5: 136, 6: 2, 8: 90, 9: 157}, 92: {0: 41, 1: 50, 2: 56, 3: 65, 4: 84, 5: 23, 6: 157, 7: 7, 8: 5, 9: 155}, 93: {0: 4, 1: 83, 2: 25, 3: 207, 4: 196}, 94: {0: 2, 1: 28, 2: 69, 3: 26, 4: 47, 5: 78, 6: 126, 7: 18, 8: 75, 9: 44}, 95: {0: 9, 1: 10, 2: 46, 3: 12, 4: 32, 5: 6, 6: 176, 7: 108, 8: 37, 9: 78}, 96: {0: 2, 1: 50, 2: 46, 3: 3, 4: 66, 5: 211, 6: 82, 7: 52}, 97: {0: 14, 1: 3, 2: 20, 3: 37, 4: 58, 5: 5, 6: 42, 7: 1, 8: 160, 9: 155}, 98: {0: 12, 1: 12, 2: 274, 3: 9, 5: 9, 7: 3, 8: 83, 9: 217}, 99: {0: 11, 1: 143, 2: 169, 3: 4, 4: 6, 5: 174, 6: 1, 8: 1}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:train_dl_global number = 195\n",
            "INFO:root:test_dl_global number = 39\n",
            "INFO:root:client_idx = 0, local_sample_number = 521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 0, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 1, local_sample_number = 772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 1, batch_num_train_local = 3, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 2, local_sample_number = 176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 2, batch_num_train_local = 0, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 3, local_sample_number = 418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 3, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 4, local_sample_number = 569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 4, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 5, local_sample_number = 583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 5, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 6, local_sample_number = 474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 6, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 7, local_sample_number = 208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 7, batch_num_train_local = 0, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 8, local_sample_number = 434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 8, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 9, local_sample_number = 504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 9, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 10, local_sample_number = 518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 10, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 11, local_sample_number = 522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 11, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 12, local_sample_number = 402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 12, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 13, local_sample_number = 629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 13, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 14, local_sample_number = 625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 14, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 15, local_sample_number = 576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 15, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 16, local_sample_number = 349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 16, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 17, local_sample_number = 271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 17, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 18, local_sample_number = 309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 18, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 19, local_sample_number = 625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 19, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 20, local_sample_number = 168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 20, batch_num_train_local = 0, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 21, local_sample_number = 571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 21, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 22, local_sample_number = 587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 22, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 23, local_sample_number = 572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 23, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 24, local_sample_number = 1008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 24, batch_num_train_local = 3, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 25, local_sample_number = 709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 25, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 26, local_sample_number = 537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 26, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 27, local_sample_number = 575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 27, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 28, local_sample_number = 501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 28, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 29, local_sample_number = 553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 29, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 30, local_sample_number = 582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 30, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 31, local_sample_number = 512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 31, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 32, local_sample_number = 506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 32, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 33, local_sample_number = 607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 33, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 34, local_sample_number = 717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 34, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 35, local_sample_number = 539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 35, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 36, local_sample_number = 572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 36, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 37, local_sample_number = 536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 37, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 38, local_sample_number = 565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 38, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 39, local_sample_number = 328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 39, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 40, local_sample_number = 770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 40, batch_num_train_local = 3, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 41, local_sample_number = 648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 41, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 42, local_sample_number = 449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 42, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 43, local_sample_number = 283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 43, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 44, local_sample_number = 325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 44, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 45, local_sample_number = 335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 45, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 46, local_sample_number = 581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 46, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 47, local_sample_number = 367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 47, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 48, local_sample_number = 508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 48, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 49, local_sample_number = 540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 49, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 50, local_sample_number = 527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 50, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 51, local_sample_number = 591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 51, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 52, local_sample_number = 631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 52, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 53, local_sample_number = 299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 53, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 54, local_sample_number = 442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 54, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 55, local_sample_number = 405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 55, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 56, local_sample_number = 665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 56, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 57, local_sample_number = 513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 57, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 58, local_sample_number = 464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 58, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 59, local_sample_number = 635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 59, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 60, local_sample_number = 531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 60, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 61, local_sample_number = 517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 61, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 62, local_sample_number = 570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 62, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 63, local_sample_number = 611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 63, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 64, local_sample_number = 141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 64, batch_num_train_local = 0, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 65, local_sample_number = 216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 65, batch_num_train_local = 0, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 66, local_sample_number = 264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 66, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 67, local_sample_number = 293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 67, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 68, local_sample_number = 532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 68, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 69, local_sample_number = 457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 69, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 70, local_sample_number = 446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 70, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 71, local_sample_number = 226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 71, batch_num_train_local = 0, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 72, local_sample_number = 582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 72, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 73, local_sample_number = 368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 73, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 74, local_sample_number = 820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 74, batch_num_train_local = 3, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 75, local_sample_number = 659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 75, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 76, local_sample_number = 592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 76, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 77, local_sample_number = 491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 77, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 78, local_sample_number = 636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 78, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 79, local_sample_number = 532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 79, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 80, local_sample_number = 322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 80, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 81, local_sample_number = 553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 81, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 82, local_sample_number = 334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 82, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 83, local_sample_number = 662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 83, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 84, local_sample_number = 320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 84, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 85, local_sample_number = 557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 85, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 86, local_sample_number = 434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 86, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 87, local_sample_number = 455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 87, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 88, local_sample_number = 289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 88, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 89, local_sample_number = 487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 89, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 90, local_sample_number = 506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 90, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 91, local_sample_number = 569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 91, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 92, local_sample_number = 643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 92, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 93, local_sample_number = 515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 93, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 94, local_sample_number = 513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 94, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 95, local_sample_number = 514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 95, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 96, local_sample_number = 512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 96, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 97, local_sample_number = 495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 97, batch_num_train_local = 1, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 98, local_sample_number = 619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 98, batch_num_train_local = 2, batch_num_test_local = 39\n",
            "INFO:root:client_idx = 99, local_sample_number = 509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download = True\n",
            "Files already downloaded and verified\n",
            "download = True\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:client_idx = 99, batch_num_train_local = 1, batch_num_test_local = 39\n"
          ]
        }
      ],
      "source": [
        "def init_training_device(process_ID, fl_worker_num, gpu_num_per_machine):\n",
        "  \"\"\"\n",
        "    The function maps the process ID to GPU ID: <process ID, GPU ID>\n",
        "  \"\"\"\n",
        "  if process_ID == 0:\n",
        "      device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      return device\n",
        "\n",
        "  process_gpu_dict = dict()\n",
        "  for client_index in range(fl_worker_num):\n",
        "      gpu_index = client_index % gpu_num_per_machine\n",
        "      process_gpu_dict[client_index] = gpu_index\n",
        "\n",
        "  logging.info(process_gpu_dict)\n",
        "  device = torch.device(\"cuda:\" + str(process_gpu_dict[process_ID - 1]) if torch.cuda.is_available() else \"cpu\")\n",
        "  logging.info(device)\n",
        "  return device\n",
        "\n",
        "def load_data(args, dataset_name):\n",
        "    \"\"\"\n",
        "      :param (str) dataset_name: name of the dataset used CIFAR-10\n",
        "      :return: (list) dataset: dataloaders splitted according with input parameters\n",
        "    \"\"\"\n",
        "    data_loader = load_partition_data_cifar10\n",
        "    # the input parameters of data_loader are defined as global variables\n",
        "    # args = [dataset,data_dir,partition_method,partition_alpha,client_number,batch_size]\n",
        "    train_data_num, test_data_num, train_data_global, test_data_global, train_data_local_num_dict,  \\\n",
        "    train_data_local_dict, test_data_local_dict, class_num = data_loader(args.dataset, args.data_dir, \\\n",
        "                                                                         args.partition_method, args.partition_alpha, \\\n",
        "                                                                         args.client_number, args.batch_size)\n",
        "\n",
        "    dataset = [train_data_num, test_data_num, train_data_global, test_data_global,\n",
        "               train_data_local_num_dict, train_data_local_dict, test_data_local_dict, class_num]\n",
        "    return dataset\n",
        "\n",
        "# Device initialization\n",
        "device = init_training_device(process_ID=0, fl_worker_num=1, gpu_num_per_machine= args.gpu_num_per_server)\n",
        "\n",
        "# Loading data.\n",
        "# Note: if you use # of client epoch larger than 1,\n",
        "# please set the shuffle=False for the dataloader (CIFAR10/CIFAR100/CINIC10),\n",
        "# which keeps the batch sequence order across epoches.\n",
        "dataset = load_data(args, args.dataset)\n",
        "[train_data_num, test_data_num, train_data_global, test_data_global,\n",
        "  train_data_local_num_dict, train_data_local_dict, test_data_local_dict, class_num] = dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpuIbuM0Xgw3"
      },
      "source": [
        "## Client/Server Models definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4knEj6_0P4S0"
      },
      "source": [
        "### ResNet Server model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xg20TQcP4ZA"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "ResNet for CIFAR-10/100 Dataset.\n",
        "Reference:\n",
        "1. https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "2. https://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua\n",
        "3. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "Deep Residual Learning for Image Recognition. https://arxiv.org/abs/1512.03385\n",
        "'''\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "__all__ = ['ResNet']\n",
        "\n",
        "\n",
        "class MyGroupNorm(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(MyGroupNorm, self).__init__()\n",
        "        self.norm = nn.GroupNorm(num_groups=2, num_channels=num_channels,eps=1e-5, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "    \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_server(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10, zero_init_residual=False, groups=1,\n",
        "                 width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, KD=False):\n",
        "        super(ResNet_server, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # self.maxpool = nn.MaxPool2d()\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "        self.KD = KD\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.conv1(x)\n",
        "        # x = self.bn1(x)\n",
        "        # x = self.relu(x)  # B x 16 x 32 x 32\n",
        "        # x = self.maxpool(x)\n",
        "        x = self.layer1(x)  # B x 16 x 32 x 32\n",
        "        x = self.layer2(x)  # B x 32 x 16 x 16\n",
        "        x = self.layer3(x)  # B x 64 x 8 x 8\n",
        "\n",
        "        x = self.avgpool(x)  # B x 64 x 1 x 1\n",
        "        x_f = x.view(x.size(0), -1)  # B x 64\n",
        "        x = self.fc(x_f)  # B x num_classes\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet56_server(c, pretrained=False, path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-110 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained.\n",
        "    \"\"\"\n",
        "    logging.info(\"path = \" + str(path))\n",
        "    model = ResNet_server(Bottleneck, [6, 6, 6], num_classes=c, **kwargs)\n",
        "    if pretrained:\n",
        "        checkpoint = torch.load(path)\n",
        "        state_dict = checkpoint['state_dict']\n",
        "\n",
        "        from collections import OrderedDict\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            # name = k[7:]  # remove 'module.' of dataparallel\n",
        "            name = k.replace(\"module.\", \"\")\n",
        "            new_state_dict[name] = v\n",
        "\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet49_server(c, pretrained=False, path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-49 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained.\n",
        "    \"\"\"\n",
        "    logging.info(\"path = \" + str(path))\n",
        "    model = ResNet_server(Bottleneck, [5, 5, 6], num_classes=c, **kwargs)\n",
        "    if pretrained:\n",
        "        checkpoint = torch.load(path)\n",
        "        state_dict = checkpoint['state_dict']\n",
        "\n",
        "        from collections import OrderedDict\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            # name = k[7:]  # remove 'module.' of dataparallel\n",
        "            name = k.replace(\"module.\", \"\")\n",
        "            new_state_dict[name] = v\n",
        "\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5BmI8ypQhaa"
      },
      "source": [
        "### ResNet Client model definition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao5N4OodQpQh"
      },
      "source": [
        "#### Client model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_HAAbrJQpyB"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "ResNet for CIFAR-10/100 Dataset.\n",
        "Reference:\n",
        "1. https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "2. https://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua\n",
        "3. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "Deep Residual Learning for Image Recognition. https://arxiv.org/abs/1512.03385\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "__all__ = ['ResNet']\n",
        "\n",
        "\n",
        "class MyGroupNorm(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(MyGroupNorm, self).__init__()\n",
        "        self.norm = nn.GroupNorm(num_groups=2, num_channels=num_channels,eps=1e-5, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_client(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10, zero_init_residual=False, groups=1,\n",
        "                 width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, KD=False):\n",
        "        super(ResNet_client, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 16\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # initialization is defined here:https://github.com/pytorch/pytorch/tree/master/torch/nn/modules\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)  # init: kaiming_uniform\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        # self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        # self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "        self.fc = nn.Linear(16 * block.expansion, num_classes)\n",
        "        # self.fc = nn.Linear(32 * block.expansion, num_classes)\n",
        "\n",
        "        self.KD = KD\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)  # B x 16 x 32 x 32\n",
        "        # x = self.maxpool(x)\n",
        "        extracted_features = x\n",
        "\n",
        "        x = self.layer1(x)  # B x 16 x 32 x 32\n",
        "        # x = self.layer2(x)  # B x 32 x 16 x 16\n",
        "        # x = self.layer3(x)  # B x 64 x 8 x 8\n",
        "\n",
        "        x = self.avgpool(x)  # B x 64 x 1 x 1\n",
        "        x_f = x.view(x.size(0), -1)  # B x 64\n",
        "        logits = self.fc(x_f)  # B x num_classes\n",
        "        return logits, extracted_features\n",
        "\n",
        "\n",
        "def resnet5_56(c, pretrained=False, path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-32 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained.\n",
        "    \"\"\"\n",
        "\n",
        "    model = ResNet_client(BasicBlock, [1, 2, 2], num_classes=c, **kwargs)\n",
        "    if pretrained:\n",
        "        checkpoint = torch.load(path)\n",
        "        state_dict = checkpoint['state_dict']\n",
        "\n",
        "        from collections import OrderedDict\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            # name = k[7:]  # remove 'module.' of dataparallel\n",
        "            name = k.replace(\"module.\", \"\")\n",
        "            new_state_dict[name] = v\n",
        "\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet8_56(c, pretrained=False, path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a ResNet-32 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model = ResNet_client(Bottleneck, [2, 2, 2], num_classes=c, **kwargs)\n",
        "    if pretrained:\n",
        "        checkpoint = torch.load(path)\n",
        "        state_dict = checkpoint['state_dict']\n",
        "\n",
        "        from collections import OrderedDict\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            # name = k[7:]  # remove 'module.' of dataparallel\n",
        "            name = k.replace(\"module.\", \"\")\n",
        "            new_state_dict[name] = v\n",
        "\n",
        "        model.load_state_dict(new_state_dict)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLxmxnI7Ow5x"
      },
      "source": [
        "### Client/Server models initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zGqK63mOxEL"
      },
      "outputs": [],
      "source": [
        "def create_client_model(args, n_classes):\n",
        "    # Uncomment the following line if you want to use BatchNorm\n",
        "    client_model = resnet8_56(n_classes)\n",
        "    # Uncommnet the following line if you want to use GroupNorm\n",
        "    # client_model = ResNet_client(Bottleneck, [2, 2, 2], num_classes=n_classes, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=MyGroupNorm)\n",
        "    \n",
        "    return client_model\n",
        "\n",
        "\n",
        "def create_server_model(n_classes):\n",
        "    # Uncomment the following line if you want to use BatchNorm\n",
        "    server_model = resnet56_server(n_classes)\n",
        "    # Uncommnet the following line if you want to use GroupNorm\n",
        "    # server_model = ResNet_server(Bottleneck,[5, 5, 6], num_classes=n_classes, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=MyGroupNorm)\n",
        "\n",
        "    return server_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul52y6YFXGIt"
      },
      "source": [
        "## Distributed Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoL9oyoZYC2H"
      },
      "source": [
        "### Utilities functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-iqesPGYDJA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def get_state_dict(file):\n",
        "    try:\n",
        "        pretrain_state_dict = torch.load(file)\n",
        "    except AssertionError:\n",
        "        pretrain_state_dict = torch.load(file, map_location=lambda storage, location: storage)\n",
        "    return pretrain_state_dict\n",
        "\n",
        "\n",
        "def get_flat_params_from(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.data.view(-1))\n",
        "\n",
        "    flat_params = torch.cat(params)\n",
        "    return flat_params\n",
        "\n",
        "\n",
        "def set_flat_params_to(model, flat_params):\n",
        "    prev_ind = 0\n",
        "    for param in model.parameters():\n",
        "        flat_size = int(np.prod(list(param.size())))\n",
        "        param.data.copy_(\n",
        "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
        "        prev_ind += flat_size\n",
        "\n",
        "\n",
        "class RunningAverage():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "\n",
        "    def value(self):\n",
        "        # print(self.total)\n",
        "        # print(self.steps)\n",
        "        return self.total / float(self.steps)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, dim=1, largest=True, sorted=True)\n",
        "    # Transpose the tensor\n",
        "    pred = pred.t()\n",
        "    # Computes element-wise equality\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        # print(correct[:k].reshape(-1).sum(0))\n",
        "        correct_k = correct[:k].reshape(-1).sum(0)\n",
        "        tmp = correct_k.mul(100.0 / batch_size)\n",
        "        res.append(tmp)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "class KL_Loss(nn.Module):\n",
        "    def __init__(self, temperature=1):\n",
        "        super(KL_Loss, self).__init__()\n",
        "        self.T = temperature\n",
        "\n",
        "    def forward(self, output_batch, teacher_outputs):\n",
        "        # output_batch  -> B X num_classes\n",
        "        # teacher_outputs -> B X num_classes\n",
        "\n",
        "        # loss_2 = -torch.sum(torch.sum(torch.mul(F.log_softmax(teacher_outputs,dim=1), F.softmax(teacher_outputs,dim=1)+10**(-7))))/teacher_outputs.size(0)\n",
        "        # print('loss H:',loss_2)\n",
        "\n",
        "        output_batch = F.log_softmax(output_batch / self.T, dim=1)\n",
        "        teacher_outputs = F.softmax(teacher_outputs / self.T, dim=1) + 10 ** (-7)\n",
        "\n",
        "        loss = self.T * self.T * nn.KLDivLoss(reduction='batchmean')(output_batch, teacher_outputs)\n",
        "\n",
        "        # Same result KL-loss implementation\n",
        "        # loss = T * T * torch.sum(torch.sum(torch.mul(teacher_outputs, torch.log(teacher_outputs) - output_batch)))/teacher_outputs.size(0)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CE_Loss(nn.Module):\n",
        "    def __init__(self, temperature=1):\n",
        "        super(CE_Loss, self).__init__()\n",
        "        self.T = temperature\n",
        "\n",
        "    def forward(self, output_batch, teacher_outputs):\n",
        "        # output_batch      -> B X num_classes\n",
        "        # teacher_outputs   -> B X num_classes\n",
        "\n",
        "        output_batch = F.log_softmax(output_batch / self.T, dim=1)\n",
        "        teacher_outputs = F.softmax(teacher_outputs / self.T, dim=1)\n",
        "\n",
        "        # Same result CE-loss implementation torch.sum -> sum of all element\n",
        "        loss = -self.T * self.T * torch.sum(torch.mul(output_batch, teacher_outputs)) / teacher_outputs.size(0)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "def save_dict_to_json(d, json_path):\n",
        "    \"\"\"Saves dict of floats in json file\n",
        "    Args:\n",
        "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
        "        json_path: (string) path to json file\n",
        "    \"\"\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
        "        d = {k: v for k, v in d.items()}\n",
        "        json.dump(d, f, indent=4)\n",
        "\n",
        "\n",
        "# Filter out batch norm parameters and remove them from weight decay - gets us higher accuracy 93.2 -> 93.48\n",
        "# https://arxiv.org/pdf/1807.11205.pdf\n",
        "def bnwd_optim_params(model, model_params, master_params):\n",
        "    bn_params, remaining_params = split_bn_params(model, model_params, master_params)\n",
        "    return [{'params': bn_params, 'weight_decay': 0}, {'params': remaining_params}]\n",
        "\n",
        "\n",
        "def split_bn_params(model, model_params, master_params):\n",
        "    def get_bn_params(module):\n",
        "        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): return module.parameters()\n",
        "        accum = set()\n",
        "        for child in module.children(): [accum.add(p) for p in get_bn_params(child)]\n",
        "        return accum\n",
        "\n",
        "    mod_bn_params = get_bn_params(model)\n",
        "    zipped_params = list(zip(model_params, master_params))\n",
        "\n",
        "    mas_bn_params = [p_mast for p_mod, p_mast in zipped_params if p_mod in mod_bn_params]\n",
        "    mas_rem_params = [p_mast for p_mod, p_mast in zipped_params if p_mod not in mod_bn_params]\n",
        "    return mas_bn_params, mas_rem_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcdjrGfaX6DW"
      },
      "source": [
        "###  GKTServerTrainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlYnf1VhX6Hc"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "class GKTServerTrainer(object):\n",
        "    def __init__(self, client_num, device, server_model, args):\n",
        "        self.client_num = client_num\n",
        "        self.device = device\n",
        "        self.args = args\n",
        "\n",
        "        \"\"\"\n",
        "            when use data parallel, we should increase the batch size accordingly (single GPU = 64; 4 GPUs = 256)\n",
        "            One epoch training time: single GPU (64) = 1:03; 4 x GPUs (256) = 38s; 4 x GPUs (64) = 1:00\n",
        "            Note that if we keep the same batch size, the frequent GPU-CPU-GPU communication will lead to\n",
        "            slower training than a single GPU.\n",
        "        \"\"\"\n",
        "        # server model\n",
        "        self.model_global = server_model\n",
        "\n",
        "        if args.multi_gpu_server and torch.cuda.device_count() > 1:\n",
        "            self.model_global = nn.DataParallel(self.model_global, device_ids=[0, 1, 2, 3]).to(device)\n",
        "\n",
        "        self.model_global.train()\n",
        "        self.model_global.to(self.device)\n",
        "\n",
        "        self.model_params = self.master_params = self.model_global.parameters()\n",
        "\n",
        "        optim_params = bnwd_optim_params(self.model_global, self.model_params,\n",
        "                                               self.master_params) if args.no_bn_wd else self.master_params\n",
        "\n",
        "        if self.args.optimizer == \"SGD\":\n",
        "            self.optimizer = torch.optim.SGD(optim_params, lr=self.args.lr, momentum=0.9,\n",
        "                                             nesterov=True,\n",
        "                                             weight_decay=self.args.wd)\n",
        "        elif self.args.optimizer == \"Adam\":\n",
        "            self.optimizer = optim.Adam(optim_params, lr=self.args.lr, weight_decay=0.0001, amsgrad=True)\n",
        "\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'max')\n",
        "\n",
        "        self.criterion_CE = nn.CrossEntropyLoss()\n",
        "        self.criterion_KL = KL_Loss(self.args.temperature)\n",
        "        self.best_acc = 0.0\n",
        "\n",
        "        # key: client_index; value: extracted_feature_dict\n",
        "        self.client_extracted_feauture_dict = dict()\n",
        "\n",
        "        # key: client_index; value: logits_dict\n",
        "        self.client_logits_dict = dict()\n",
        "\n",
        "        # key: client_index; value: labels_dict\n",
        "        self.client_labels_dict = dict()\n",
        "\n",
        "        # key: client_index; value: labels_dict\n",
        "        self.server_logits_dict = dict()\n",
        "\n",
        "        # for test\n",
        "        self.client_extracted_feauture_dict_test = dict()\n",
        "        self.client_labels_dict_test = dict()\n",
        "\n",
        "        self.model_dict = dict()\n",
        "        self.sample_num_dict = dict()\n",
        "        self.train_acc_list = []\n",
        "        self.train_loss_list = []\n",
        "        self.test_acc_list = []\n",
        "        self.test_acc_avg = 0.0\n",
        "        self.test_loss_avg = 0.0\n",
        "\n",
        "        self.flag_client_model_uploaded_dict = dict()\n",
        "        for idx in range(self.client_num):\n",
        "            self.flag_client_model_uploaded_dict[idx] = False\n",
        "\n",
        "    def add_local_trained_result(self, index, extracted_feature_dict, logits_dict, labels_dict,\n",
        "                                 extracted_feature_dict_test, labels_dict_test):\n",
        "        logging.info(\"add_model. index = %d\" % index)\n",
        "        self.client_extracted_feauture_dict[index] = extracted_feature_dict\n",
        "        self.client_logits_dict[index] = logits_dict\n",
        "        self.client_labels_dict[index] = labels_dict\n",
        "        self.client_extracted_feauture_dict_test[index] = extracted_feature_dict_test\n",
        "        self.client_labels_dict_test[index] = labels_dict_test\n",
        "\n",
        "        self.flag_client_model_uploaded_dict[index] = True\n",
        "\n",
        "    def check_whether_all_receive(self):\n",
        "        for idx in range(self.client_num):\n",
        "            if not self.flag_client_model_uploaded_dict[idx]:\n",
        "                return False\n",
        "        for idx in range(self.client_num):\n",
        "            self.flag_client_model_uploaded_dict[idx] = False\n",
        "        return True\n",
        "\n",
        "    def get_global_logits(self, client_index):\n",
        "        return self.server_logits_dict[client_index]\n",
        "\n",
        "    def train(self, round_idx):\n",
        "        if self.args.sweep == 1:\n",
        "            self.sweep(round_idx)\n",
        "        else:\n",
        "            if self.args.whether_training_on_client == 1:\n",
        "                self.train_and_distill_on_client(round_idx)\n",
        "            else:\n",
        "                self.do_not_train_on_client(round_idx)\n",
        "\n",
        "    def train_and_distill_on_client(self, round_idx):\n",
        "        if self.args.test:\n",
        "            epochs_server, whether_distill_back = self.get_server_epoch_strategy_test()\n",
        "        else:\n",
        "            if self.args.client_model == \"resnet56\":\n",
        "                epochs_server, whether_distill_back = self.get_server_epoch_strategy_reset56_2(round_idx)\n",
        "            else:\n",
        "                epochs_server = self.args.epochs_server\n",
        "\n",
        "        # train according to the logits from the client\n",
        "        self.train_and_eval(round_idx, epochs_server)\n",
        "\n",
        "        # adjust the learning rate based on the number of epochs.\n",
        "        # https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "        self.scheduler.step(self.best_acc, epoch=round_idx)\n",
        "\n",
        "    def do_not_train_on_client(self, round_idx):\n",
        "        self.train_and_eval(round_idx, 1)\n",
        "        self.scheduler.step(self.best_acc, epoch=round_idx)\n",
        "\n",
        "    def sweep(self, round_idx):\n",
        "        # train according to the logits from the client\n",
        "        self.train_and_eval(round_idx, self.args.epochs_server)\n",
        "        self.scheduler.step(self.best_acc, epoch=round_idx)\n",
        "\n",
        "    def get_server_epoch_strategy_test(self):\n",
        "        return 1, True\n",
        "\n",
        "    # ResNet56\n",
        "    def get_server_epoch_strategy_reset56(self, round_idx):\n",
        "        whether_distill_back = True\n",
        "        # set the training strategy\n",
        "        if round_idx < 20:\n",
        "            epochs = 20\n",
        "        elif 20 <= round_idx < 30:\n",
        "            epochs = 15\n",
        "        elif 30 <= round_idx < 40:\n",
        "            epochs = 10\n",
        "        elif 40 <= round_idx < 50:\n",
        "            epochs = 5\n",
        "        elif 50 <= round_idx < 100:\n",
        "            epochs = 5\n",
        "        elif 100 <= round_idx < 150:\n",
        "            epochs = 3\n",
        "        elif 150 <= round_idx <= 200:\n",
        "            epochs = 2\n",
        "            whether_distill_back = False\n",
        "        else:\n",
        "            epochs = 1\n",
        "            whether_distill_back = False\n",
        "        return epochs, whether_distill_back\n",
        "\n",
        "    # ResNet56-2\n",
        "    def get_server_epoch_strategy_reset56_2(self, round_idx):\n",
        "        whether_distill_back = True\n",
        "        # set the training strategy\n",
        "        epochs = self.args.epochs_server\n",
        "        return epochs, whether_distill_back\n",
        "\n",
        "    # not increase after 40 epochs\n",
        "    def get_server_epoch_strategy2(self, round_idx):\n",
        "        whether_distill_back = True\n",
        "        # set the training strategy\n",
        "        if round_idx < 20:\n",
        "            epochs = 20\n",
        "        elif 20 <= round_idx < 30:\n",
        "            epochs = 15\n",
        "        elif 30 <= round_idx < 40:\n",
        "            epochs = 10\n",
        "        elif 40 <= round_idx < 50:\n",
        "            epochs = 8\n",
        "        elif 50 <= round_idx < 100:\n",
        "            epochs = 5\n",
        "        elif 100 <= round_idx < 150:\n",
        "            epochs = 3\n",
        "        elif 150 <= round_idx <= 200:\n",
        "            epochs = 1\n",
        "            whether_distill_back = False\n",
        "        else:\n",
        "            epochs = 1\n",
        "            whether_distill_back = False\n",
        "        return epochs, whether_distill_back\n",
        "\n",
        "    def train_and_eval(self, round_idx, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            logging.info(\"train_and_eval. round_idx = %d, epoch = %d\" % (round_idx, epoch))\n",
        "            train_metrics = self.train_large_model_on_the_server()\n",
        "\n",
        "            if epoch == epochs - 1:\n",
        "                self.train_loss_list.append(train_metrics['train_loss'])\n",
        "                self.train_acc_list.append(train_metrics['train_accTop1'])\n",
        "                # Evaluate for one epoch on validation set\n",
        "                test_metrics = self.eval_large_model_on_the_server()\n",
        "\n",
        "                # Find the best accTop1 model.\n",
        "                test_acc = test_metrics['test_accTop1']\n",
        "                self.test_acc_list.append(test_acc)\n",
        "\n",
        "                last_path = os.path.join('/content/checkpoint/last.ph')\n",
        "                # Save latest model weights, optimizer and accuracy\n",
        "                torch.save({'state_dict': self.model_global.state_dict(),\n",
        "                            'optim_dict': self.optimizer.state_dict(),\n",
        "                            'epoch': round_idx + 1,\n",
        "                            'test_accTop1': test_metrics['test_accTop1'],\n",
        "                            'test_accTop5': test_metrics['test_accTop5']}, last_path)\n",
        "\n",
        "                # If best_eval, best_save_path\n",
        "                is_best = test_acc >= self.best_acc\n",
        "                if is_best:\n",
        "                    logging.info(\"- Found better accuracy\")\n",
        "                    self.best_acc = test_acc\n",
        "                    # Save best metrics in a json file in the model directory\n",
        "                    test_metrics['epoch'] = round_idx + 1\n",
        "                    \n",
        "                    save_dict_to_json(test_metrics, os.path.join('/content/checkpoint/', \"test_best_metrics.json\"))\n",
        "\n",
        "                    # Save model and optimizer\n",
        "                    shutil.copyfile(last_path, os.path.join('/content/checkpoint/', 'best.pth'))\n",
        "\n",
        "    def train_large_model_on_the_server(self):\n",
        "        # clear the server side logits\n",
        "        for key in self.server_logits_dict.keys():\n",
        "            self.server_logits_dict[key].clear()\n",
        "        self.server_logits_dict.clear()\n",
        "        # Set the model in the \"train mode\" is not a call to the method train of the class\n",
        "        self.model_global.train()\n",
        "\n",
        "        loss_avg = RunningAverage()\n",
        "        accTop1_avg = RunningAverage()\n",
        "        accTop5_avg = RunningAverage()\n",
        "\n",
        "        for client_index in self.client_extracted_feauture_dict.keys():\n",
        "            extracted_feature_dict = self.client_extracted_feauture_dict[client_index]\n",
        "            logits_dict = self.client_logits_dict[client_index]\n",
        "            labels_dict = self.client_labels_dict[client_index]\n",
        "\n",
        "            s_logits_dict = dict()\n",
        "            self.server_logits_dict[client_index] = s_logits_dict\n",
        "            for batch_index in extracted_feature_dict.keys():\n",
        "                batch_feature_map_x = torch.from_numpy(extracted_feature_dict[batch_index]).to(self.device)\n",
        "                batch_logits = torch.from_numpy(logits_dict[batch_index]).float().to(self.device)\n",
        "                batch_labels = torch.from_numpy(labels_dict[batch_index]).long().to(self.device)\n",
        "\n",
        "                # logging.info(\"running: batch_index = %d, client_index = %d\" % (batch_index, client_index))\n",
        "                output_batch = self.model_global(batch_feature_map_x)\n",
        "\n",
        "                if self.args.whether_distill_on_the_server == 1:\n",
        "                    loss_kd = self.criterion_KL(output_batch, batch_logits).to(self.device)\n",
        "                    loss_true = self.criterion_CE(output_batch, batch_labels).to(self.device)\n",
        "                    loss = loss_kd + self.args.alpha * loss_true\n",
        "                else:\n",
        "                    loss_true = self.criterion_CE(output_batch, batch_labels).to(self.device)\n",
        "                    loss = loss_true\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update average loss and accuracy\n",
        "                metrics = accuracy(output_batch, batch_labels, topk=(1, 5))\n",
        "                accTop1_avg.update(metrics[0].item())\n",
        "                accTop5_avg.update(metrics[1].item())\n",
        "                loss_avg.update(loss.item())\n",
        "\n",
        "                # update the logits for each client\n",
        "                # Note that this must be running in the model.train() model,\n",
        "                # since the client will continue the iteration based on the server logits.\n",
        "                s_logits_dict[batch_index] = output_batch.cpu().detach().numpy()\n",
        "\n",
        "        # compute mean of all metrics in summary\n",
        "        train_metrics = {'train_loss': loss_avg.value(),\n",
        "                         'train_accTop1': accTop1_avg.value(),\n",
        "                         'train_accTop5': accTop5_avg.value()}\n",
        "\n",
        "        metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in train_metrics.items())\n",
        "        logging.info(\"- Train metrics: \" + metrics_string)\n",
        "        return train_metrics\n",
        "\n",
        "    def eval_large_model_on_the_server(self):\n",
        "\n",
        "        # set model to evaluation mode\n",
        "        self.model_global.eval()\n",
        "        loss_avg = RunningAverage()\n",
        "        accTop1_avg = RunningAverage()\n",
        "        accTop5_avg = RunningAverage()\n",
        "        with torch.no_grad():\n",
        "            for client_index in self.client_extracted_feauture_dict_test.keys():\n",
        "                extracted_feature_dict = self.client_extracted_feauture_dict_test[client_index]\n",
        "                labels_dict = self.client_labels_dict_test[client_index]\n",
        "\n",
        "                for batch_index in extracted_feature_dict.keys():\n",
        "                    batch_feature_map_x = torch.from_numpy(extracted_feature_dict[batch_index]).to(self.device)\n",
        "                    batch_labels = torch.from_numpy(labels_dict[batch_index]).long().to(self.device)\n",
        "\n",
        "                    output_batch = self.model_global(batch_feature_map_x)\n",
        "                    loss = self.criterion_CE(output_batch, batch_labels)\n",
        "\n",
        "                    # Update average loss and accuracy\n",
        "                    metrics = accuracy(output_batch, batch_labels, topk=(1, 5))\n",
        "                    # only one element tensors can be converted to Python scalars\n",
        "                    accTop1_avg.update(metrics[0].item())\n",
        "                    accTop5_avg.update(metrics[1].item())\n",
        "                    loss_avg.update(loss.item())\n",
        "\n",
        "        # compute mean of all metrics in summary\n",
        "        test_metrics = {'test_loss': loss_avg.value(),\n",
        "                        'test_accTop1': accTop1_avg.value(),\n",
        "                        'test_accTop5': accTop5_avg.value()}\n",
        "\n",
        "        metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in test_metrics.items())\n",
        "        logging.info(\"- Test  metrics: \" + metrics_string)\n",
        "        return test_metrics\n",
        "    \n",
        "\n",
        "    def plot_train_loss_vs_rounds(self):\n",
        "        \"\"\"\n",
        "            Plot the loss vs communication rounds\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        plt.figure()\n",
        "        rounds = np.arange(1, len(self.train_loss_list) + 1)\n",
        "        plt.plot(rounds, self.train_loss_list)\n",
        "        plt.title(\"server loss vs communication rounds\")\n",
        "        plt.savefig(\"server_loss_x_round\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_train_accuracy_vs_rounds(self):\n",
        "        \"\"\"\n",
        "            Plot the accuracy vs communication rounds\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        plt.figure()\n",
        "        rounds = np.arange(1, len(self.train_acc_list) + 1)\n",
        "        plt.plot(rounds, self.train_acc_list)\n",
        "        plt.title(\"server train accuracy vs communication rounds\")\n",
        "        plt.savefig(\"server_accuracy_x_round\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_test_accuracy_vs_rounds(self):\n",
        "        \"\"\"\n",
        "            Plot the accuracy vs communication rounds\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        plt.figure()\n",
        "        rounds = np.arange(1, len(self.test_acc_list) + 1)\n",
        "        plt.plot(rounds, self.test_acc_list)\n",
        "        plt.title(\"server test accuracy vs communication rounds\")\n",
        "        plt.savefig(\"server_test_accuracy_x_round\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4NyfuEnaHSh"
      },
      "source": [
        "### GKTClientTrainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StNTwXuiaFL2"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class GKTClientTrainer(object):\n",
        "    def __init__(self, client_index, local_training_data, local_test_data, local_sample_number, device,\n",
        "                 client_model, args):\n",
        "        self.client_index = client_index\n",
        "        self.local_training_data = local_training_data[client_index]\n",
        "        self.local_test_data = local_test_data[client_index]\n",
        "\n",
        "        self.local_sample_number = local_sample_number\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        self.device = device\n",
        "        self.client_model = client_model\n",
        "\n",
        "        logging.info(\"client device = \" + str(self.device))\n",
        "        self.client_model.to(self.device)\n",
        "\n",
        "        self.model_params = self.master_params = self.client_model.parameters()\n",
        "\n",
        "        optim_params = bnwd_optim_params(self.client_model, self.model_params,\n",
        "                                               self.master_params) if args.no_bn_wd else self.master_params\n",
        "\n",
        "        if self.args.optimizer == \"SGD\":\n",
        "            self.optimizer = torch.optim.SGD(optim_params, lr=self.args.lr, momentum=0.9,\n",
        "                                             nesterov=True,\n",
        "                                             weight_decay=self.args.wd)\n",
        "        elif self.args.optimizer == \"Adam\":\n",
        "            self.optimizer = optim.Adam(optim_params, lr=self.args.lr, weight_decay=0.0001, amsgrad=True)\n",
        "\n",
        "        self.criterion_CE = nn.CrossEntropyLoss()\n",
        "        self.criterion_KL = KL_Loss(self.args.temperature)\n",
        "\n",
        "        self.server_logits_dict = dict()\n",
        "        self.round_loss = []\n",
        "\n",
        "    def get_sample_number(self):\n",
        "        return self.local_sample_number\n",
        "\n",
        "    def update_large_model_logits(self, logits):\n",
        "        self.server_logits_dict = logits\n",
        "\n",
        "    def train(self):\n",
        "        # key: batch_index; value: extracted_feature_map\n",
        "        extracted_feature_dict = dict()\n",
        "\n",
        "        # key: batch_index; value: logits\n",
        "        logits_dict = dict()\n",
        "\n",
        "        # key: batch_index; value: label\n",
        "        labels_dict = dict()\n",
        "\n",
        "        # for test - key: batch_index; value: extracted_feature_map\n",
        "        extracted_feature_dict_test = dict()\n",
        "        labels_dict_test = dict()\n",
        "\n",
        "        if self.args.whether_training_on_client == 1:\n",
        "            self.client_model.train()\n",
        "            # train and update\n",
        "            epoch_loss = []\n",
        "            for epoch in range(self.args.epochs_client):\n",
        "                batch_loss = []\n",
        "                for batch_idx, (images, labels) in enumerate(self.local_training_data):\n",
        "                    images = Variable(images.to(self.device))\n",
        "                    # I don't know why we have to do such conversion here...but It works..so...\n",
        "                    labels = torch.tensor(labels, dtype=torch.long, device=self.device)\n",
        "                    self.optimizer.zero_grad()\n",
        "                    # logging.info(\"shape = \" + str(images.shape))\n",
        "                    log_probs, _ = self.client_model(images)\n",
        "                    loss_true = self.criterion_CE(log_probs, labels)\n",
        "                    if len(self.server_logits_dict) != 0:\n",
        "                        large_model_logits = torch.from_numpy(self.server_logits_dict[batch_idx]).to(\n",
        "                            self.device)\n",
        "                        loss_kd = self.criterion_KL(log_probs, large_model_logits)\n",
        "                        loss = loss_true + self.args.alpha * loss_kd\n",
        "                    else:\n",
        "                        loss = loss_true\n",
        "\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    # Try to comment this section\n",
        "                    \"\"\"\n",
        "                    logging.info('client {} - Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                        self.client_index, epoch, batch_idx * len(images), len(self.local_training_data.dataset),\n",
        "                                                  100. * batch_idx / len(self.local_training_data), loss.item()))\n",
        "                    \"\"\"\n",
        "                    batch_loss.append(loss.item())\n",
        "\n",
        "                # Since we are considering just one epoch on the local clients, the epoch loss \n",
        "                # coincides with \"round loss\" or loss per round\n",
        "                self.round_loss.append(sum(batch_loss) / len(batch_loss))\n",
        "                # used for local computation               \n",
        "                epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
        "                logging.info('client {} - Update Epoch: {} \\tEpoch Loss: {:.6f}'.format(\n",
        "                    self.client_index, epoch, sum(batch_loss) / len(batch_loss)))\n",
        "                 \n",
        "        self.client_model.eval()\n",
        "\n",
        "        \"\"\"\n",
        "            If the training dataset is too large, we may meet the following issue.\n",
        "            ===================================================================================\n",
        "            =   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\n",
        "            =   PID 28488 RUNNING AT ChaoyangHe-GPU-RTX2080Tix4\n",
        "            =   EXIT CODE: 9\n",
        "            =   CLEANING UP REMAINING PROCESSES\n",
        "            =   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES\n",
        "            ===================================================================================\n",
        "            The signal 9 may indicate that the job is out of memory.\n",
        "\n",
        "            So it is better to run this program in a 256G CPU host memory. \n",
        "            If deploying our algorithm in real world system, please optimize the memory usage by compression.\n",
        "        \"\"\"\n",
        "        # The features are extracted at the end of each local training\n",
        "        for batch_idx, (images, labels) in enumerate(self.local_training_data):\n",
        "            images = Variable(images.to(self.device))\n",
        "            # I don't know why we have to do such conversion here...but if we don't there are problems\n",
        "            labels = torch.tensor(labels, dtype=torch.long, device=self.device)\n",
        "            # logging.info(\"shape = \" + str(images.shape))\n",
        "            log_probs, extracted_features = self.client_model(images)\n",
        "\n",
        "            # logging.info(\"shape = \" + str(extracted_features.shape))\n",
        "            # logging.info(\"element size = \" + str(extracted_features.element_size()))\n",
        "            # logging.info(\"nelement = \" + str(extracted_features.nelement()))\n",
        "            # logging.info(\"GPU memory1 = \" + str(extracted_features.nelement() * extracted_features.element_size()))\n",
        "            extracted_feature_dict[batch_idx] = extracted_features.cpu().detach().numpy()\n",
        "            log_probs = log_probs.cpu().detach().numpy()\n",
        "            logits_dict[batch_idx] = log_probs\n",
        "            labels_dict[batch_idx] = labels.cpu().detach().numpy()\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(self.local_test_data):\n",
        "            # test_images, test_labels = images.to(self.device), labels.to(self.device)\n",
        "            test_images = Variable(images.to(self.device))\n",
        "            # I don't know why we have to do such conversion here...but It works..so...\n",
        "            test_labels = torch.tensor(labels, dtype=torch.long, device=self.device)\n",
        "            _, extracted_features_test = self.client_model(test_images)\n",
        "            extracted_feature_dict_test[batch_idx] = extracted_features_test.cpu().detach().numpy()\n",
        "            labels_dict_test[batch_idx] = test_labels.cpu().detach().numpy()\n",
        "\n",
        "        return extracted_feature_dict, logits_dict, labels_dict, extracted_feature_dict_test, labels_dict_test\n",
        "    \n",
        "    def plot_loss_vs_rounds(self):\n",
        "        \"\"\"\n",
        "            Plot the loss vs communication rounds\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        import numpy as np\n",
        "        sns.set()\n",
        "        rounds = np.arange(len(self.round_loss))\n",
        "        plt.plot(rounds, self.round_loss)\n",
        "        plt.title(\"client loss vs communication rounds\")\n",
        "        plt.savefig(\"client\" + str(self.client_index) + \"_loss\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6AhQNKVbLZB"
      },
      "source": [
        "### FedML distributed training init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBoBZ3_2XGNk"
      },
      "outputs": [],
      "source": [
        "def init_server(args, device, comm, rank, size, model):\n",
        "    # aggregator\n",
        "    client_num = size - 1\n",
        "    server_trainer = GKTServerTrainer(client_num, device, model, args)\n",
        "  \n",
        "    return server_trainer\n",
        "\n",
        "def init_client(args, device, comm, process_id, size, model, train_data_local_dict, test_data_local_dict,\n",
        "                train_data_local_num_dict):\n",
        "    client_ID = process_id - 1\n",
        "\n",
        "    # 2. initialize the trainer\n",
        "    trainer = GKTClientTrainer(client_ID, train_data_local_dict, test_data_local_dict, \\\n",
        "                               train_data_local_num_dict,device, model, args)\n",
        "    return trainer\n",
        "\n",
        "def FedML_FedGKT_distributed(process_id, worker_number, device, comm, model, train_data_local_num_dict,\n",
        "                             train_data_local_dict, test_data_local_dict, args):\n",
        "    if process_id == 0:\n",
        "        # Note the server doesn't take images as input data, while the trainer for the client takes as input\n",
        "        # the dictionaries containing indexes which will be used to select the images in the dataset. Indeed, the server\n",
        "        # will be trained on the feature maps sent by the clients (We have to define a \"protocol\")\n",
        "        trainer = init_server(args, device, comm, process_id, worker_number, model)\n",
        "\n",
        "    else:\n",
        "        trainer = init_client(args, device, comm, process_id, worker_number, model, train_data_local_dict, \\\n",
        "                              test_data_local_dict, train_data_local_num_dict)\n",
        "        \n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaPf9zJYObZE"
      },
      "source": [
        "## Main function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "284eI3HyOczz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "fbeb4e9a-9e2d-4623-d0b8-e9bc31423b38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b8644c45358b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocess_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_number\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# List containing the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ],
      "source": [
        "# create models\n",
        "process_ids = [i for i in range(args.client_number + 1)]\n",
        "size = len(process_ids)\n",
        "# List containing the models\n",
        "models = []\n",
        "for i in range(len(process_ids)):\n",
        "    if i == 0:\n",
        "        models.append(create_server_model(class_num))\n",
        "        print(\"Server model: \", models[i])\n",
        "        # numel method returns the total number of elements in the input tensor.\n",
        "        print(f\"Parameters per layer: {[p.numel() for p in models[i].parameters()]} \")\n",
        "        print(f\"Total number of parameters: {sum([p.numel() for p in models[i].parameters()])}\")\n",
        "    else:\n",
        "        # create client model\n",
        "        models.append(create_client_model(args, class_num))\n",
        "print(\"Client model: \", models[1])\n",
        "# numel method returns the total number of elements in the input tensor.\n",
        "print(f\"Parameters per layer: {[p.numel() for p in models[1].parameters()]} \")\n",
        "print(f\"Total number of parameters: {sum([p.numel() for p in models[1].parameters()])}\")\n",
        "\n",
        "# Initialize the trainers, trainers[0] will be the server.\n",
        "# Basically, the list will contain the Trainer object associated with each model.\n",
        "trainers = []\n",
        "for j in range(size):\n",
        "    # process_id = j\n",
        "    # worker_number = \"our\" size ==> len(process_ids)\n",
        "    # FedML_FedGKT_distributed(process_id, worker_number, device, comm, model, train_data_local_num_dict,\n",
        "    #                          train_data_local_dict, test_data_local_dict, args)\n",
        "    trainer = FedML_FedGKT_distributed(process_id=j, worker_number=size, device=device, comm=None, \\\n",
        "                                       model=models[j], train_data_local_num_dict=train_data_local_num_dict, \\\n",
        "                                       train_data_local_dict=train_data_local_dict, \\\n",
        "                                       test_data_local_dict=test_data_local_dict, args=args)\n",
        "    trainers.append(trainer)\n",
        "# Argument of train should be the number of communication round.\n",
        "print(\"START TRAINING\")\n",
        "\n",
        "# Simulate the distributed training framework\n",
        "for curr_round in range(args.comm_round):\n",
        "  # We random pick 10 clients among the total, index 0 is reserved for the server\n",
        "  clients_idx = np.random.choice(np.arange(1, len(trainers) + 1), size=10, replace=False)\n",
        "  # trainers[0] will be always the server trainer\n",
        "  for k in range(len(clients_idx) + 1, 0, -1):\n",
        "    # indexes will go from len(clients_idx) + 1 included to 0 excluded, so we have to fix the index\n",
        "    if (k - 1) != 0:\n",
        "      # Train the clients for a number of local_epochs specified in the parameters\n",
        "      trainer_idx = clients_idx[k - 2]\n",
        "      # Structures that will be used to enable the alternating training. The function returns dictionaries, where\n",
        "      # Key: Client id, Value = tensors containing the information used by the server to perform the training\n",
        "      extracted_feature_dict, logits_dict, labels_dict, extracted_feature_dict_test, labels_dict_test = \\\n",
        "          trainers[trainer_idx].train()\n",
        "\n",
        "      # Update the local results on the server\n",
        "      # we index the clients in the range [0, len(clients_idx) - 1]\n",
        "      trainers[0].add_local_trained_result(index = k - 2, extracted_feature_dict=extracted_feature_dict,\n",
        "                                            logits_dict=logits_dict, labels_dict=labels_dict,\n",
        "                                            extracted_feature_dict_test=extracted_feature_dict_test,\n",
        "                                            labels_dict_test=labels_dict_test)\n",
        "    else:\n",
        "      # Server Training\n",
        "      trainers[0].train(curr_round)\n",
        "      # Once the server finishes its current training, we have to update the clients\n",
        "      # with the global logits\n",
        "      for idx in range(len(clients_idx), 0, -1):\n",
        "        trainer_idx = clients_idx[idx - 1]\n",
        "        # Updates the global logit to clients\n",
        "        global_logit = trainers[0].get_global_logits(client_index=idx - 1)\n",
        "        trainers[trainer_idx].update_large_model_logits(logits=global_logit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots\n"
      ],
      "metadata": {
        "id": "u_Q7Z087k2wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clients plots\n",
        "trainers[1].plot_loss_vs_rounds()\n",
        "trainers[2].plot_loss_vs_rounds()\n",
        "trainers[3].plot_loss_vs_rounds()\n",
        "trainers[4].plot_loss_vs_rounds()\n",
        "# Server plots\n",
        "trainers[0].plot_train_loss_vs_rounds()\n",
        "trainers[0].plot_train_accuracy_vs_rounds()\n",
        "trainers[0].plot_test_accuracy_vs_rounds()"
      ],
      "metadata": {
        "id": "ClKVIN9kk2Tw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WePKs5Hdi9SG",
        "SdpeKkcHXVoV",
        "GsjBz-BWdxlR",
        "cpuIbuM0Xgw3",
        "pLxmxnI7Ow5x",
        "Ul52y6YFXGIt",
        "YoL9oyoZYC2H",
        "VcdjrGfaX6DW",
        "s4NyfuEnaHSh",
        "d6AhQNKVbLZB"
      ],
      "name": "Point 3 -FedGKT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68f14a0e3ef34785ad7c45e4c9e611a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3711899d965c480188b327e0911b1934",
              "IPY_MODEL_4806bb2f379e45e2ae9dbb933815d19a",
              "IPY_MODEL_2f26ca9dbbe04d79b1140df43ddb90c8"
            ],
            "layout": "IPY_MODEL_97d6a7c41535463a8f5a886b846c276b"
          }
        },
        "3711899d965c480188b327e0911b1934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce994710836f4daba76e8f39b184b8d3",
            "placeholder": "",
            "style": "IPY_MODEL_bac1b63264f54e67b76efd714af51156",
            "value": ""
          }
        },
        "4806bb2f379e45e2ae9dbb933815d19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d98484143843d09799844b81061597",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d106d88d32f54eefbf4e3737752ab6bf",
            "value": 170498071
          }
        },
        "2f26ca9dbbe04d79b1140df43ddb90c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32aaa31f04d5450dba4a0925521a4dee",
            "placeholder": "",
            "style": "IPY_MODEL_fbc4685cb7c149d4973aa962e84136ca",
            "value": " 170499072/? [00:06&lt;00:00, 26453393.50it/s]"
          }
        },
        "97d6a7c41535463a8f5a886b846c276b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce994710836f4daba76e8f39b184b8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac1b63264f54e67b76efd714af51156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d98484143843d09799844b81061597": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d106d88d32f54eefbf4e3737752ab6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32aaa31f04d5450dba4a0925521a4dee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc4685cb7c149d4973aa962e84136ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}