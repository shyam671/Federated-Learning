# -*- coding: utf-8 -*-
"""Point 1 - Centralized training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qemqjTWiFTxsLfQTurM_j2SlXrO3lrEy

#Centralized training: define the upper bound
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all the necessary libraries
import os
import torch
import torchvision
import tarfile
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from torchvision.datasets.utils import download_url
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torchvision.utils import make_grid
from torch.autograd import Variable
import torch.optim as optim
from torchvision import models
from torchvision import transforms as tt
from PIL import Image
import random
import numpy as np
import matplotlib.pyplot as plt
import time
import collections


# set manual seed for reproducibility
# [100, 0, 42] => list of seeds
seed = 0

# general reproducibility
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

# gpu training specific
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

from sklearn.model_selection import train_test_split
# Data transforms (normalization + data augmentation)
# Correct values for mean and std for normalization (normalization is done per channel)
stats = ((0.49139968, 0.48215841, 0.44653091),(0.24703223, 0.24348513, 0.26158784))
# Apply transformations to train dataset
train_transform = tt.Compose([tt.ToTensor(),
                              tt.RandomCrop(32, padding=4,padding_mode='reflect'),
                              tt.RandomHorizontalFlip(p=0.5),
                              tt.Normalize(*stats)])
# Apply transformations to test set, we have done evaluated on of the original 32Ã—32 image without transformations.
test_transform = tt.Compose([tt.ToTensor(),
                             tt.Normalize(*stats)])
# Let's try first with no transformations, then we will pass the object above in the parameter transform
# 1. Define train and test datasets
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)
validation_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=test_transform, download=True)
batch_size = 256
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
validation_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size)

print(isinstance(train_loader, collections.Iterable))
print(isinstance(validation_loader, collections.Iterable))
print(len(train_dataset))
print(len(validation_dataset))
# print(len(test_dataset))

"""## Try to display images inside a batch
A subset of the images inside a batch, just 10 images in order to understand if the transformations work smoothly.
"""

def inv_normalization(im):
    inv_normalize = tt.Normalize(
        mean = [ - 0.4914 / 0.247, - 0.4822 / 0.243, - 0.4465 / 0.261],
        std=[1 / 0.247, 1 / 0.243, 1 / 0.261]
    )
    im_inv = inv_normalize(im)
    return im_inv


def display_batch(train_loader):
    images, labels = next(iter(train_loader))
    print(images.shape)
    im = make_grid(images[:20], nrow=5)
    im_inv = inv_normalization(im)

"""## Network Architecture + Model Definition

### Group Normalization Layer
"""

class MyGroupNorm(nn.Module):
    def __init__(self, num_channels):
        super(MyGroupNorm, self).__init__()
        self.norm = nn.GroupNorm(num_groups=2, num_channels=num_channels,
                                 eps=1e-5, affine=True)
    
    def forward(self, x):
        x = self.norm(x)
        return x

"""## Model definition"""

# 3. Model definition with BatchNormalization layers
BN_model = models.resnet50(pretrained=False, num_classes=10)
print(BN_model)
# numel method returns the total number of elements in the input tensor.
print(f"Parameters per layer: {[p.numel() for p in BN_model.parameters()]} ")
print(f"Total number of parameters: {sum([p.numel() for p in BN_model.parameters()])}")

# Model definition with GroupNormalization layers
GN_model = models.resnet.ResNet(models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=10, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=MyGroupNorm)

print(GN_model)
# numel method returns the total number of elements in the input tensor.
print(f"Parameters per layer: {[p.numel() for p in GN_model.parameters()]} ")
print(f"Total number of parameters: {sum([p.numel() for p in GN_model.parameters()])}")

"""## Loss definition, Optimizer definition"""

# 4. Loss defintion
criterion = nn.CrossEntropyLoss()
epochs = 150
# 5. Optimizer defintion 
learning_rate = 1e-4
max_learning_rate = 1e-3
weight_decay = 1e-4
# betas=(0.9, 0.999) default values for Adam optmizer
BN_optimizer = optim.Adam(BN_model.parameters(), lr=learning_rate)
GN_optimizer = optim.Adam(GN_model.parameters(), lr=learning_rate)

#BN_optimizer = optim.SGD(BN_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)
#GN_optimizer = optim.SGD(GN_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)
# Scheduler for decreasing the learning rate
BN_scheduler = torch.optim.lr_scheduler.OneCycleLR(BN_optimizer, max_lr=max_learning_rate, epochs=epochs, steps_per_epoch=len(train_loader))
GN_scheduler = torch.optim.lr_scheduler.OneCycleLR(GN_optimizer, max_lr=max_learning_rate, epochs=epochs, steps_per_epoch=len(train_loader))

"""## Training Loop"""

import seaborn as sns
sns.set()

def train_vs_validation_loss(epochs, train_losses, val_losses):
  # VISUALIZATIONS: TRAIN LOSS VS VALIDATION LOSS
  # LEARNING CURVE
  plt.figure()
  plt.plot(range(epochs), train_losses, label='train loss')
  plt.plot(range(epochs), val_losses, label='val loss')
  plt.legend()
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.title("TRAIN LOSS VS VALIDATION LOSS")
  plt.savefig("/content/learning_curve.png")
  plt.show()

def training_loop(model,optimizer,epochs,scheduler):
  MODEL_SAVED = 'model_.pt'
  # VERY BASIC TRAINING LOOP
  epochs = epochs 
  # Gradient clipping: Apart from the layer weights and outputs, it also helpful to limit the values of 
  # gradients to a small range to prevent undesirable changes in parameters due to large gradient values. 
  # This simple yet effective technique is called gradient clipping. 
  # Learn more: https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48
  # We set grad_clip = 0.1 later
  grad_clip = True
  # Move the model on the GPU
  assert torch.cuda.device_count() > 0, "Fail: No GPU device detected"
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  model = model.to(device)
  train_losses = []
  val_losses = []

  # Start the training loop
  for epoch in range(epochs):
    # Set the model in "training mode"
    model.train()
    running_training_loss = 0.0
    running_val_loss = 0.0
    correct = 0
    for i, (images, labels) in enumerate(train_loader):
      current_batch = len(images)
      # Put images on the GPU
      images = Variable(images.to(device))
      labels = Variable(labels.to(device))
      # Reset the gradients
      optimizer.zero_grad()
      # Take the outputs
      outputs = model(images)
      # Calculate the loss
      loss = criterion(outputs, labels)
      # Calculate the gradients
      loss.backward()
      # Try Gradient Clipping
      if grad_clip: 
          nn.utils.clip_grad_value_(model.parameters(), clip_value=0.1)
      # Updates the parameters
      optimizer.step()
      # Try OneCycle Scheduler
      scheduler.step()
      # Weighted AVG = loss * batch_size / len(dataset)
      running_training_loss += loss.item() * current_batch
    # In the end we store one loss per epoch...
    train_losses.append(running_training_loss / len(train_dataset))
    # Set the network in evaluation mode
    model.eval()
    # VALIDATION PHASE
    with torch.no_grad():
      for j, (val_images, val_labels) in enumerate(validation_loader):
        current_batch = len(val_labels)
        # Send the batch to the GPU
        val_images = Variable(val_images.to(device))
        val_labels = Variable(val_labels.to(device))
        # Get the outputs
        val_output = model(val_images)
        # Loss evaluation
        val_loss = criterion(val_output, val_labels)
        running_val_loss += val_loss.item() * current_batch
        # Get the prediction for the accuracy
        _, prediction = torch.max(val_output, 1)
        # The result of the masking is a boolean, but we can sum the values, since True = 1, and False = 0
        correct += (prediction.cpu() == val_labels.cpu()).sum()
            
    
    # Also one validation loss per epoch is stored
    val_losses.append(running_val_loss / len(validation_dataset))
    accuracy = 100 * (correct / len(validation_dataset)) # Note: validation_dataset, not validation_loader
    print(
          f"Epoch:({epoch + 1}/{epochs})" 
          f" - Training Loss: {running_training_loss / len(train_dataset):.5e} "
          f" - Validation Loss: {running_val_loss / len(validation_dataset):.5e}"
          f" - Training Accuracy: {accuracy:.3e}"
          f" - lr: {optimizer.state_dict()['param_groups'][0]['lr']:.2e}")
  
    # CHECKPOINT  
    torch.save({
      #'epoch': epoch,
      'model_state_dict': model.state_dict(),
      #'optimizer_state_dict': optimizer.state_dict(),
      #'scheduler_state_dict': scheduler.state_dict(),
      #'training_loss_x_epoch': train_losses,
      #'validation_loss_x_epoch': val_losses,

    }, MODEL_SAVED)
      
  print(f"total time {(time.time() - start_time) / 60 :.2f} minutes")
  #train_vs_validation_loss(epochs, train_losses, val_losses)
  return model

# Start 
start_time = time.time()
BN_model = training_loop(model=BN_model, optimizer=BN_optimizer, epochs=epochs, scheduler=BN_scheduler)
GN_model = training_loop(model=GN_model, optimizer=GN_optimizer, epochs=epochs, scheduler=GN_scheduler)

"""## Model Evaluation on the test set"""

BN_model.eval()
GN_model.eval()
# VALIDATION PHASE
with torch.no_grad():
  BN_test_correct = 0
  GN_test_correct = 0
  for j, (test_images, test_labels) in enumerate(test_loader):
    current_batch = len(test_labels)
    # Send the batch to the GPU
    test_images = Variable(test_images.cuda())
    test_labels = Variable(test_labels.cuda())
    # Get the outputs
    BN_test_output = BN_model(test_images)
    GN_test_output = GN_model(test_images)
    # Get the prediction for the accuracy
    _, BN_test_prediction = torch.max(BN_test_output, 1)
    _, GN_test_prediction = torch.max(GN_test_output, 1)
    # The result of the masking is a boolean, but we can sum the values, since True = 1, and False = 0
    BN_test_correct += (BN_test_prediction.cpu() == test_labels.cpu()).sum()
    GN_test_correct += (GN_test_prediction.cpu() == test_labels.cpu()).sum()
  
BN_test_accuracy = 100 * (BN_test_correct / len(test_dataset))
GN_test_accuracy = 100 * (GN_test_correct / len(test_dataset))

print(f"BN Test Accuracy = {BN_test_accuracy:.3e}")
print(f"GN Test Accuracy = {GN_test_accuracy:.3e}")
